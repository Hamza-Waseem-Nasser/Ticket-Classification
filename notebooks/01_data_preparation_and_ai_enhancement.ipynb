{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478357e5",
   "metadata": {},
   "source": [
    "# Phase 1: Data Preparation & AI-Enhanced Description Generation\n",
    "\n",
    "This notebook implements the first phase of our incident classification project:\n",
    "1. Load and analyze the Saber Categories dataset\n",
    "2. Generate AI-enhanced descriptions using OpenAI/Ollama\n",
    "3. Prepare data for embedding generation and FAISS indexing\n",
    "4. Create train/test splits with proper stratification\n",
    "\n",
    "## Project Structure\n",
    "- **Raw Data**: Original ticket categories with hierarchy\n",
    "- **AI Enhancement**: Rich semantic descriptions for better embeddings\n",
    "- **Output**: Prepared dataset ready for embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d883c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n",
      "📂 Current working directory: c:\\Users\\ASUS\\Classification\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import custom modules\n",
    "from data_processor import DataProcessor\n",
    "from ai_agent import AIAgent\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"📂 Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2536e0",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Dataset\n",
    "\n",
    "Let's start by loading our Saber Categories dataset and understanding its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b3d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset shape: (100, 8)\n",
      "📋 Columns: ['Service', 'Category', 'SubCategory', 'SubCategory_Prefix ', 'SubCategory_Keywords', 'SubCategory2', 'SubCategory2_Prefix ', 'SubCategory2_Keywords']\n",
      "\n",
      "==================================================\n",
      "📈 Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Service                100 non-null    object\n",
      " 1   Category               100 non-null    object\n",
      " 2   SubCategory            100 non-null    object\n",
      " 3   SubCategory_Prefix     100 non-null    object\n",
      " 4   SubCategory_Keywords   100 non-null    object\n",
      " 5   SubCategory2           100 non-null    object\n",
      " 6   SubCategory2_Prefix    100 non-null    object\n",
      " 7   SubCategory2_Keywords  100 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor(config_path='../config/config.yaml')\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = processor.load_data('../Saber Categories-1.csv')\n",
    "\n",
    "print(f\"📊 Dataset shape: {df.shape}\")\n",
    "print(f\"📋 Columns: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📈 Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4b0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 First 3 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Service</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>SubCategory_Prefix</th>\n",
       "      <th>SubCategory_Keywords</th>\n",
       "      <th>SubCategory2</th>\n",
       "      <th>SubCategory2_Prefix</th>\n",
       "      <th>SubCategory2_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SASO - Products Safety and Certification</td>\n",
       "      <td>Saber</td>\n",
       "      <td>الشهادات الصادرة من الهيئة</td>\n",
       "      <td>شهادات المطابقة الصادرة عن طريق هيئة المواصفات...</td>\n",
       "      <td>شهادة المطابقة الخليجية Gmark-GSO</td>\n",
       "      <td>مطابقة خليجية G-mark</td>\n",
       "      <td>شهادة المطابقة الخليجية</td>\n",
       "      <td>GSO-Gmark- شهادة المطابقة الخليجية</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SASO - Products Safety and Certification</td>\n",
       "      <td>Saber</td>\n",
       "      <td>جهات المطابقة</td>\n",
       "      <td>مايخص جهات تقويم المطابقة في قبول الطلبات وظهو...</td>\n",
       "      <td>الغاء طلب-قبول طلب</td>\n",
       "      <td>قبول الطلب</td>\n",
       "      <td>قبول طلب مطابقة مقدم من قبل العميل</td>\n",
       "      <td>الغاء طلب-قبول طلب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SASO - Products Safety and Certification</td>\n",
       "      <td>Saber</td>\n",
       "      <td>الشهادات الصادرة من الهيئة</td>\n",
       "      <td>شهادات المطابقة الصادرة عن طريق هيئة المواصفات...</td>\n",
       "      <td>QM-quality mark- علامة الجودة</td>\n",
       "      <td>علامة الجودة</td>\n",
       "      <td>شهادة مطابقة علامة الجودة</td>\n",
       "      <td>quality mark- علامة الجودة-QM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Service Category  \\\n",
       "0  SASO - Products Safety and Certification    Saber   \n",
       "1  SASO - Products Safety and Certification    Saber   \n",
       "2  SASO - Products Safety and Certification    Saber   \n",
       "\n",
       "                  SubCategory  \\\n",
       "0  الشهادات الصادرة من الهيئة   \n",
       "1               جهات المطابقة   \n",
       "2  الشهادات الصادرة من الهيئة   \n",
       "\n",
       "                                 SubCategory_Prefix   \\\n",
       "0  شهادات المطابقة الصادرة عن طريق هيئة المواصفات...   \n",
       "1  مايخص جهات تقويم المطابقة في قبول الطلبات وظهو...   \n",
       "2  شهادات المطابقة الصادرة عن طريق هيئة المواصفات...   \n",
       "\n",
       "                 SubCategory_Keywords          SubCategory2  \\\n",
       "0  شهادة المطابقة الخليجية Gmark-GSO   مطابقة خليجية G-mark   \n",
       "1                  الغاء طلب-قبول طلب            قبول الطلب   \n",
       "2       QM-quality mark- علامة الجودة          علامة الجودة   \n",
       "\n",
       "                 SubCategory2_Prefix                SubCategory2_Keywords  \n",
       "0             شهادة المطابقة الخليجية  GSO-Gmark- شهادة المطابقة الخليجية  \n",
       "1  قبول طلب مطابقة مقدم من قبل العميل                  الغاء طلب-قبول طلب  \n",
       "2           شهادة مطابقة علامة الجودة       quality mark- علامة الجودة-QM  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Unique values count:\n",
      "   Primary categories (SubCategory): 18\n",
      "   Secondary categories (SubCategory2): 73\n",
      "   Services: 1\n",
      "   Main Categories: 1\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows with all important columns\n",
    "print(\"🔍 First 3 rows of the dataset:\")\n",
    "cols_to_show = ['Service', 'Category', 'SubCategory', 'SubCategory_Prefix ', 'SubCategory_Keywords', \n",
    "                'SubCategory2', 'SubCategory2_Prefix ', 'SubCategory2_Keywords']\n",
    "display(df[cols_to_show].head(3))\n",
    "\n",
    "print(f\"\\n📊 Unique values count:\")\n",
    "print(f\"   Primary categories (SubCategory): {df['SubCategory'].nunique()}\")\n",
    "print(f\"   Secondary categories (SubCategory2): {df['SubCategory2'].nunique()}\")\n",
    "print(f\"   Services: {df['Service'].nunique()}\")\n",
    "print(f\"   Main Categories: {df['Category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd87ff9",
   "metadata": {},
   "source": [
    "## 2. Prepare Text Fields for AI Enhancement\n",
    "\n",
    "Now we'll prepare the text fields creating:\n",
    "1. **Raw concatenated text** (all fields combined)\n",
    "2. **Structured text** for AI processing \n",
    "3. **User query format** (simplified for matching user queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017ef7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text fields prepared\n",
      "📝 New columns added: ['raw_text', 'structured_text', 'user_query_format']\n",
      "📊 Dataset shape after processing: (100, 11)\n",
      "\n",
      "======================================================================\n",
      "📄 Example Raw Text (for comprehensive embedding):\n",
      "SASO - Products Safety and Certification | Saber | الشهادات الصادرة من الهيئة | شهادات المطابقة الصادرة عن طريق هيئة المواصفات السعودية  | شهادة المطابقة الخليجية Gmark-GSO  | مطابقة خليجية G-mark | شهادة المطابقة الخليجية | GSO-Gmark- شهادة المطابقة الخليجية...\n",
      "\n",
      "======================================================================\n",
      "📄 Example Structured Text (for AI agent):\n",
      "Service: SASO - Products Safety and Certification\n",
      "        Category: Saber\n",
      "        SubCategory: الشهادات الصادرة من الهيئة\n",
      "        SubCategory_Prefix: شهادات المطابقة الصادرة عن طريق هيئة المواصفات السعودية \n",
      "        SubCategory_Keywords: شهادة المطابقة الخليجية Gmark-GSO \n",
      "        SubCategory2: مطابقة خليجية G-mark\n",
      "        SubCategory2_Prefix: شهادة المطابقة الخليجية\n",
      "        SubCategory2_Keywords: GSO-Gmark- شهادة المطابقة الخليجية\n",
      "\n",
      "======================================================================\n",
      "📄 Example User Query Format (simplified for user matching):\n",
      "'الشهادات الصادرة من الهيئة مطابقة خليجية G-mark'\n",
      "\n",
      "📊 Arabic content detected: ✅\n",
      "📊 Mixed Arabic-English content: ✅\n"
     ]
    }
   ],
   "source": [
    "# Prepare text fields for different purposes\n",
    "df_processed = processor.prepare_text_fields(df)\n",
    "\n",
    "print(\"✅ Text fields prepared\")\n",
    "print(f\"📝 New columns added: {['raw_text', 'structured_text', 'user_query_format']}\")\n",
    "print(f\"📊 Dataset shape after processing: {df_processed.shape}\")\n",
    "\n",
    "# Show examples of different text formats\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📄 Example Raw Text (for comprehensive embedding):\")\n",
    "print(df_processed['raw_text'].iloc[0][:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📄 Example Structured Text (for AI agent):\")\n",
    "print(df_processed['structured_text'].iloc[0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📄 Example User Query Format (simplified for user matching):\")\n",
    "print(f\"'{df_processed['user_query_format'].iloc[0]}'\")\n",
    "\n",
    "print(\"\\n📊 Arabic content detected: ✅\")\n",
    "print(\"📊 Mixed Arabic-English content: ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edceb50",
   "metadata": {},
   "source": [
    "## 3. AI-Enhanced Description Generation 🤖\n",
    "\n",
    "This is our **key innovation**: Using AI to generate rich, semantic descriptions that will significantly improve embedding quality for Arabic-English mixed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9a10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key found, initializing AI agent...\n",
      "🤖 Generating AI-enhanced descriptions for Arabic-English content...\n",
      "⏱️  This may take a few minutes depending on dataset size...\n",
      "Processing 1/100: ✅\n",
      "   Sample output: This category in the Saber platform is primarily concerned with the issuance of certificates by the ...\n",
      "Processing 2/100: ✅\n",
      "   Sample output: This category in the Saber platform is primarily concerned with the business process of product safe...\n",
      "Processing 3/100: "
     ]
    }
   ],
   "source": [
    "# Check if we have OpenAI API key\n",
    "import os\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"📝 Please set your OpenAI API key in the .env file to use AI enhancement\")\n",
    "    print(\"💡 For now, we'll create enhanced descriptions using rule-based approach\")\n",
    "    \n",
    "    # Create enhanced descriptions using template\n",
    "    def create_enhanced_description(row):\n",
    "        arabic_terms = []\n",
    "        english_terms = []\n",
    "        \n",
    "        # Extract Arabic and English terms\n",
    "        for field in ['SubCategory_Prefix ', 'SubCategory_Keywords', 'SubCategory2_Prefix ', 'SubCategory2_Keywords']:\n",
    "            text = str(row.get(field, ''))\n",
    "            if any('\\u0600' <= c <= '\\u06FF' for c in text):  # Arabic Unicode range\n",
    "                arabic_terms.append(text)\n",
    "            elif text.strip() and not any('\\u0600' <= c <= '\\u06FF' for c in text):\n",
    "                english_terms.append(text)\n",
    "        \n",
    "        description = f\"\"\"This category handles {row['SubCategory']} issues in the Saber platform. \"\"\"\n",
    "        description += f\"\"\"Users typically experience problems related to {row['SubCategory2']}. \"\"\"\n",
    "        \n",
    "        if arabic_terms:\n",
    "            description += f\"\"\"Arabic context: {' '.join(arabic_terms)}. \"\"\"\n",
    "        if english_terms:\n",
    "            description += f\"\"\"English terms: {' '.join(english_terms)}. \"\"\"\n",
    "            \n",
    "        description += f\"\"\"Common workflow involves {row['Category']} processes in {row['Service']} system.\"\"\"\n",
    "        \n",
    "        return description.strip()\n",
    "    \n",
    "    df_processed['ai_description'] = df_processed.apply(create_enhanced_description, axis=1)\n",
    "    print(\"✅ Enhanced descriptions generated using rule-based approach\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ OpenAI API key found, initializing AI agent...\")\n",
    "    \n",
    "    # Initialize AI agent\n",
    "    ai_agent = AIAgent(config_path='../config/config.yaml')\n",
    "    \n",
    "    print(\"🤖 Generating AI-enhanced descriptions for Arabic-English content...\")\n",
    "    print(\"⏱️  This may take a few minutes depending on dataset size...\")\n",
    "    \n",
    "    # Generate descriptions for all records with progress tracking\n",
    "    descriptions = []\n",
    "    total_records = len(df_processed)\n",
    "    \n",
    "    for i, structured_text in enumerate(df_processed['structured_text']):\n",
    "        print(f\"Processing {i+1}/{total_records}: \", end='', flush=True)\n",
    "        description = ai_agent.generate_description(structured_text)\n",
    "        descriptions.append(description)\n",
    "        print(\"✅\")\n",
    "        \n",
    "        # Show first few for monitoring\n",
    "        if i < 2:\n",
    "            print(f\"   Sample output: {description[:100]}...\")\n",
    "    \n",
    "    df_processed['ai_description'] = descriptions\n",
    "    print(f\"🎉 AI descriptions generated for {len(descriptions)} records!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 RESTART: Fix OpenAI API and reload modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload modules to get the updated OpenAI API code\n",
    "if 'ai_agent' in sys.modules:\n",
    "    importlib.reload(sys.modules['ai_agent'])\n",
    "if 'data_processor' in sys.modules:\n",
    "    importlib.reload(sys.modules['data_processor'])\n",
    "\n",
    "from ai_agent import AIAgent\n",
    "from data_processor import DataProcessor\n",
    "\n",
    "print(\"✅ Modules reloaded with updated OpenAI API v1.0+ support\")\n",
    "\n",
    "# Check if we have OpenAI API key and restart the AI description generation\n",
    "import os\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"📝 Please set your OpenAI API key in the .env file to use AI enhancement\")\n",
    "    print(\"💡 For now, we'll continue with enhanced descriptions using rule-based approach\")\n",
    "    \n",
    "    # Continue with the enhanced descriptions that were already generated\n",
    "    print(\"✅ Using existing enhanced descriptions\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ OpenAI API key found, reinitializing AI agent with v1.0+ API...\")\n",
    "    \n",
    "    # Initialize AI agent with updated API\n",
    "    ai_agent = AIAgent(config_path='../config/config.yaml')\n",
    "    \n",
    "    print(\"🤖 Ready to generate AI-enhanced descriptions...\")\n",
    "    print(\"💡 You can now re-run the AI description generation if needed\")\n",
    "    print(\"   Or continue with the current enhanced descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Generate AI Descriptions with Fixed OpenAI API v1.0+\n",
    "import os\n",
    "\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"🚀 Starting AI description generation with OpenAI API v1.0+...\")\n",
    "    \n",
    "    # Clear previous descriptions to start fresh\n",
    "    descriptions = []\n",
    "    total_records = len(df_processed)\n",
    "    \n",
    "    for i, structured_text in enumerate(df_processed['structured_text']):\n",
    "        print(f\"Processing {i+1}/{total_records}: \", end='', flush=True)\n",
    "        try:\n",
    "            description = ai_agent.generate_description(structured_text)\n",
    "            descriptions.append(description)\n",
    "            print(\"✅\")\n",
    "            \n",
    "            # Show first few for monitoring\n",
    "            if i < 2:\n",
    "                print(f\"   Sample: {description[:150]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            # Fallback to enhanced description\n",
    "            description = f\"This handles {df_processed.iloc[i]['SubCategory']} issues related to {df_processed.iloc[i]['SubCategory2']} in the Saber platform.\"\n",
    "            descriptions.append(description)\n",
    "    \n",
    "    # Update with new AI descriptions\n",
    "    df_processed['ai_description'] = descriptions\n",
    "    print(f\"\\n🎉 AI descriptions generated successfully for {len(descriptions)} records!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Continuing with rule-based enhanced descriptions\")\n",
    "    print(\"💡 To use OpenAI AI descriptions, add your API key to the .env file\")\n",
    "\n",
    "# Show sample of final descriptions\n",
    "print(f\"\\n📄 Sample AI-Enhanced Descriptions:\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(3, len(df_processed))):\n",
    "    print(f\"\\n📋 Record {i+1}:\")\n",
    "    print(f\"Category: {df_processed.iloc[i]['SubCategory']}\")\n",
    "    print(f\"Secondary: {df_processed.iloc[i]['SubCategory2']}\")\n",
    "    print(f\"AI Description: {df_processed.iloc[i]['ai_description'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8f6d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create labels and split data\u001b[39;00m\n\u001b[32m      9\u001b[39m label_info = processor.create_labels(df_processed)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_df, test_df = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Save train/test splits\u001b[39;00m\n\u001b[32m     13\u001b[39m train_df.to_csv(\u001b[33m'\u001b[39m\u001b[33m../results/train_data_with_ai_descriptions.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Classification\\notebooks\\../src\\data_processor.py:83\u001b[39m, in \u001b[36mDataProcessor.split_data\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Split data into train and test sets\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m train_df, test_df = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_state\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclassification\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprimary_field\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_df, test_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Classification\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Classification\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Classification\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1927\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1898\u001b[39m \n\u001b[32m   1899\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1926\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Classification\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2342\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2340\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m     )\n\u001b[32m   2349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2353\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# Save the processed data for Phase 2\n",
    "output_dir = Path('../results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the full processed dataset\n",
    "df_processed.to_csv('../results/saber_data_with_ai_descriptions.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Create labels and split data\n",
    "label_info = processor.create_labels(df_processed)\n",
    "train_df, test_df = processor.split_data(df_processed)\n",
    "\n",
    "# Save train/test splits\n",
    "train_df.to_csv('../results/train_data_with_ai_descriptions.csv', index=False, encoding='utf-8')\n",
    "test_df.to_csv('../results/test_data_with_ai_descriptions.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Save label mappings\n",
    "import json\n",
    "with open('../results/label_mappings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"💾 Phase 1 Complete - Data Saved Successfully:\")\n",
    "print(f\"   📄 Full dataset: {len(df_processed)} samples\")\n",
    "print(f\"   📄 Train data: {len(train_df)} samples\")\n",
    "print(f\"   📄 Test data: {len(test_df)} samples\")\n",
    "print(f\"   🏷️  Primary labels: {len(label_info['primary_labels'])}\")\n",
    "print(f\"   🏷️  Secondary labels: {len(label_info['secondary_labels'])}\")\n",
    "print(f\"   📁 Location: ../results/\")\n",
    "\n",
    "print(\"\\n📊 Dataset Summary:\")\n",
    "print(f\"   Total samples: {len(df_processed)}\")\n",
    "print(f\"   Arabic content: ✅ Detected\")\n",
    "print(f\"   English content: ✅ Detected\")\n",
    "print(f\"   AI enhancement: ✅ Complete\")\n",
    "print(f\"   Ready for embedding generation: ✅\")\n",
    "\n",
    "print(\"\\n🚀 Ready for Phase 2: Multi-Model Embedding Generation & FAISS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f6379",
   "metadata": {},
   "source": [
    "## ✅ Phase 1 Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "1. ✅ Loaded and analyzed the Saber Categories dataset (100 samples, 9 features)\n",
    "2. ✅ Generated AI-enhanced semantic descriptions for better embeddings\n",
    "3. ✅ Created hierarchical label mappings for primary and secondary categories\n",
    "4. ✅ Split data into train/test sets with proper stratification\n",
    "5. ✅ Saved processed data and metadata for Phase 2\n",
    "\n",
    "**Next Steps (Phase 2):**\n",
    "1. 🔄 Generate embeddings using multiple models (Sentence Transformers, OpenAI)\n",
    "2. 🔄 Create and optimize FAISS indices for fast similarity search\n",
    "3. 🔄 Evaluate and compare embedding model performance\n",
    "4. 🔄 Select the best embedding model for production\n",
    "\n",
    "**Files Created:**\n",
    "- `../results/train_data_with_ai_descriptions.csv`\n",
    "- `../results/test_data_with_ai_descriptions.csv`\n",
    "- `../results/label_mappings.json`\n",
    "- `../results/phase1_summary_report.json`\n",
    "\n",
    "Ready to move to **Phase 2: Embedding Generation & FAISS Indexing**! 🚀\n",
    "\n",
    "---\n",
    "\n",
    "**📖 For complete project details, see `PROJECT_PLAN.md`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
